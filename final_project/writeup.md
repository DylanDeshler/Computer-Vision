### Introduction

For my final project I competed in the BELKA competition (https://www.kaggle.com/competitions/leash-BELKA/overview). The purpose of this competition is to predict if a given molecule will bind to any of three proteins (sEH, BRD4, and HSA). The sponsors provided a lot of data for this competition ~98 million molecules, so being able to crunch through data is important. The average bind rate is about 0.6% which makes this dataset pretty imbalanced. Further the test set consists of a substantial amount of out of domain samples. Each molecule provided is comprised of three building blocks, and the sponsors have made it clear that a large portion of the test set are molecules that share no building blocks with the training set. This makes it an especially challenging dataset and competition. The scoring criteria for this competition is micro average precision, as molecules can bind to multiple proteins.

## How to Represent a Molecule

There are three main ways to encode a molecule: a graph, SMILES, and Morgan Fingerprint. I tried using all three approaches in my experiments, so I will briefly explain each here.

A graph is the most straight-forward representation. Simply encode the atoms as vertices and their bonds as edges. Add any relevant information to the vertices and edges as necessary (atomic number, bond type, chirality, ring degree, etc).
The competition provides an excellent explanation of SMILES: "SMILES is a concise string notation used to represent the structure of chemical molecules. It encodes the molecular graph, including atoms, bonds, connectivity, and stereochemistry as a linear sequence of characters, by traversing the molecule graph. SMILES is widely used in machine learning applications for chemistry, such as molecular property prediction, drug discovery, and materials design, as it provides a standardized and machine-readable format for representing and manipulating chemical structures."
Here is an explanation of Morgan Fingerprints from the paper that introduced them: "In short, the Morgan fingerprint encodes the atom groups of a chemical into a binary vector with length and radius as its two parameters. The length refers to the lengths of the vector, while the radius represents the size of the atom groups. The larger the radius, the larger the atom group it can represent." (https://pubs.acs.org/doi/10.1021/acs.est.3c02198)
While each representation encodes similar information, there are clear differences between them that affect the performance and choice of algorithm used with them.

## Attempts

# PyTorch vs TensorFlow
While experimenting I found a deeply unsettling issue. Models I trained in PyTorch would fail to the trivial solution (always guessing the dominant class) while those I trained in TensorFlow would gain traction and produce reasonable results. The exact same model trained exactly the same way converges in TensorFlow and fails in PyTorch. I spent hours trying to figure out what the issue was and failed, TensorFlow is clearly doing something under the hood that helps with class imbalance that PyTorch is not. I would love to know the answer.

# GNN
I picked this challenge because it sounded interesting and I thought it would be a great opportunity to work with graph neural networks. I recently won a contract at work to build mid term weather forecasting graph neural network models (like GraphCast), and was excited to try my hand at them. I found that another competitor had already built and shared a notebook for training GNNs (https://www.kaggle.com/code/agastyapulapaka/leash-tutorial-molecular-graphs-and-gnns). I was surprised that the notebook had such middling results (at the time, now they would be considered pretty poor). I improved the network by making it deeper, increasing the connectivity, and adding more features (like chirality). This improved the performance substantially, but it still lacked behind top competitors. So I looked for answers and found researchers who had similar gripes with GNNs. What I learned was that despite their promise, GNNs have seriously 
struggled in academia and industry, so I wasn't alone! The consensus I found was that somehow GNNs are good at learning a smoothed representation but not a strong, highly non-linear predictive representation. So after a little more toying with the GNN, I decided to move on and try building a transformer instead. 

# Transformer
Transformer architectures are all the rage and for good reason, they show great results in practically every domain. I have worked with transformers for computer vision and a tiny bit for nlp, but never for anything like molecules. I built a simple transformer encoder decoder architecture and immediately ran into memory issues (no surprise given the O(n^2) computational and memory requirements for self-attention). For practically all of my experiments I've used my work's GPUs (don't tell anyone). They're far from modern but are better than what I can use for free with kaggle or collab. In my work I mostly trained Masked AutoEncoders where a substantial portion of the input image is not fed into the transformer, which drastically reduces the O(n^2) burden. But here I was feeding in 1024 length bit vectors. There are many ways to encode molecular representations, the most common being a graph, SMILE strings, and fingerprint bit vectors. I generated Extended-Connectivity Fingerprints (ECFPs) of dimension 1024 and radius 2. I take the index of each non-zero element and feed them into an embedding layer and mask away all zeros (being careful not to mask the 0th index bit). I trained transformers on this input with varying depths and dimensions, as well as learned and sinusoidal positional embeddings. No matter what I did I got no traction. That is until I switched my implementation to TensorFlow, then magically I had traction. My results were mediocre by this point in the competition. I believe that with more compute I could get great results, but not with my current hardware constraints. I even implemented some fancy non-stand transformer scaling that I found in a recent paper on 'small' LLMs (https://arxiv.org/pdf/2404.14619). Everything I tried helped and improved the performance somewhat, but not enough to compete with what would come next.

# 1D CNN
I found an implementation for a simple 1D CNN that got extremely promising results (https://www.kaggle.com/code/ahmedelfazouan/belka-1dcnn-starter-with-all-data/notebook). It was still far from the top contestants, but was significantly better than my improved GNN, transformer, and it was much simpler. I was excited to try it out because I had also implemented a 1D CNN (in PyTorch) but got terrible results. I was able to improve the 1D CNN by making it deeper, increasing the dimensionality, training for longer, and all the other classic neural network tricks. This improved network got my best performance of the competitions so far, I was 25th of nearly 800 at the time! Well within striking distance of the top 10-15, the top 5 performers are in a class of their own. I was extremely proud given the simplicity of the network, the improvements I was able to make to it, and that it was a single algorithm. Most of the people above and below me on the leaderboards were using ensemble methods, which I want to stay clear of until the end.

# MLP
I recently began training an MLP on Extended-Connectivity Fingerprints (ECFPs). It is too early to make any claims about the results, but working directly with binary inputs is difficult and I have found that the network will overfit even with surprisingly few parameters ~10k.

# GBT
Like all kaggle competitions, gradient boosted trees of all varieties dominate. The ability to generate 10 different implementations simply and ensemble them together gives pretty reasonable accuracy. I'm not a fan of GBTs so I haven't touched them in this competition, but towards the end of the competition I will likely ensemble them with all of my other attempts (transformer, gnn, cnn). It's a simple way to improve results by a few percent.

## Next Steps
I want to build  or leverage a self-supervised network (be it transformer or cnn) and use it to learn a robust molecular representation. My hope is that this representation will be much more robust to novel molecules and building blocks (which as mentioned at the beginning is critical to success in this competition). I will then fine-tune the network or learn a simple classifier on top of the frozen parameters for protein binding classification. I found a paper that did exactly this with BERT called MolBERT, the code and weights are open-sourced which would save me the time and hassle of training my own network (https://arxiv.org/pdf/2011.13230).

## Conclusion
I have spent a lot of time having a lot of fun with this competition, so much so that I may have neglected my work and scholastic responsibilities somewhat. I have learned a lot about molecules that I never hoped to learn, but now would love to apply in the future to interesting problems like protein folding and drug discovery. I will continue to compete in this competition and my hope is to win the best student prize. At the time of my best submission I was the highest ranked student, but that is no longer the case. Likely I would need to finish in the top 15 to have a reasonable chance at the prize. It is unlikely and will be hard, but it is something I'm willing to strive for. I think you should be able to check my progress/status in the competition here (https://www.kaggle.com/dylandesh/competitions).

